{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemma-header.width-1200.format-webp.webp\" width=\"100%\">\n",
    "\n",
    "## Instruct Fine-tuning [Gemma](https://blog.google/technology/developers/gemma-open-models/) using qLora and Supervise Finetuning\n",
    "\n",
    "This is a comprahensive notebook and tutorial on how to fine tune the `gemma-2b-it` Model\n",
    "\n",
    "All the code will be available on my Github. Do drop by and give a follow and a starâ­.\\\n",
    "[sagarvk24](https://github.com/sagarvk24)\n",
    "\\\n",
    "[Github Code](https://github.com/sagarvk24/Fine-Tune-LLMs-.git)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for GPU (In my case, using Kaggle's T4 x 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-04T16:59:40.874156Z",
     "iopub.status.busy": "2025-02-04T16:59:40.873784Z",
     "iopub.status.idle": "2025-02-04T16:59:41.093282Z",
     "shell.execute_reply": "2025-02-04T16:59:41.092285Z",
     "shell.execute_reply.started": "2025-02-04T16:59:40.874115Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb  4 16:59:40 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   49C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   56C    P8             11W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Install the Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T16:59:43.234815Z",
     "iopub.status.busy": "2025-02-04T16:59:43.234526Z",
     "iopub.status.idle": "2025-02-04T17:00:18.925007Z",
     "shell.execute_reply": "2025-02-04T17:00:18.924213Z",
     "shell.execute_reply.started": "2025-02-04T16:59:43.234790Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.7/115.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m279.7/279.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2023.10.0 which is incompatible.\n",
      "s3fs 2024.9.0 requires fsspec==2024.9.0.*, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install -q -U bitsandbytes==0.42.0 #To load the quantized version of the model\n",
    "!pip3 install -q -U peft==0.8.2 #For Parameter-Efficient Fine Tuning\n",
    "!pip3 install -q -U trl==0.7.10 #Supervised Fine Tuning \n",
    "!pip3 install -q -U accelerate==0.27.1\n",
    "!pip3 install -q -U datasets==2.17.0 #for loading the datasets\n",
    "!pip3 install -q -U transformers==4.38.0 #for main processing and training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load the model using QLoRA Quantization to reduce the usage of memory, as it is very difficult to load such big LLM and fine tune. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:00:25.117336Z",
     "iopub.status.busy": "2025-02-04T17:00:25.117033Z",
     "iopub.status.idle": "2025-02-04T17:00:30.627510Z",
     "shell.execute_reply": "2025-02-04T17:00:30.626603Z",
     "shell.execute_reply.started": "2025-02-04T17:00:25.117312Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, #Quantization\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 #To avoid information loss while fine tuning\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Hugging Face: Log In and Access Key Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:28:08.027128Z",
     "iopub.status.busy": "2025-02-04T17:28:08.026804Z",
     "iopub.status.idle": "2025-02-04T17:28:08.045726Z",
     "shell.execute_reply": "2025-02-04T17:28:08.044607Z",
     "shell.execute_reply.started": "2025-02-04T17:28:08.027102Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c051bed6b26e4470aeef51142b5d88f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case using Google Colab\n",
    "#Paste the access token in Colab's Environment \n",
    "\n",
    "#import os\n",
    "#from google.colab import userdata\n",
    "#os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:00:49.232150Z",
     "iopub.status.busy": "2025-02-04T17:00:49.231812Z",
     "iopub.status.idle": "2025-02-04T17:03:02.438799Z",
     "shell.execute_reply": "2025-02-04T17:03:02.438153Z",
     "shell.execute_reply.started": "2025-02-04T17:00:49.232121Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e4c0df09bf4c6a9ecf3a0e3f188c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c88ea3afbfa4ff984b8c2140de4422c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88adbb00e5bf41579b3d9579133cf2c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c90685fb144ae4a995596b62cb64c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f5c9c9b1fe4df2880d59604c8f2fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37650bfea0054f9f9d1995b1e3fa882e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4bbdc8915984d0c8e16c2ccbd68a90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968d8c3085984bee8c0b7190158cc842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5efc41340b5c4e708fd02ea179d91284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4b85eb39464ba3bf76ecf974cd3b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e76ec84ed441599ad8be73f964e5cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Difference between base model and 'it' model is that, base model is the most raw form of the LLM model. The output is produced in a very raw format, whereas 'it' model produces outputs in a very \n",
    "\n",
    "# model_id = \"google/gemma-7b-it\"\n",
    "# model_id = \"google/gemma-7b\"\n",
    "model_id = \"google/gemma-2b-it\"\n",
    "# model_id = \"google/gemma-2b\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:03:05.565179Z",
     "iopub.status.busy": "2025-02-04T17:03:05.564659Z",
     "iopub.status.idle": "2025-02-04T17:03:05.570090Z",
     "shell.execute_reply": "2025-02-04T17:03:05.569202Z",
     "shell.execute_reply.started": "2025-02-04T17:03:05.565146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_completion(query: str, model, tokenizer) -> str:\n",
    "  device = \"cuda:0\"\n",
    "\n",
    "  prompt_template = \"\"\"\n",
    "  <start_of_turn>user\n",
    "  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "  {query}\n",
    "  <end_of_turn>\\n<start_of_turn>model\n",
    "  \n",
    "\n",
    "  \"\"\"\n",
    "  prompt = prompt_template.format(query=query)\n",
    "\n",
    "  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "  model_inputs = encodeds.to(device)\n",
    "\n",
    "\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "  # decoded = tokenizer.batch_decode(generated_ids)\n",
    "  decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "  return (decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:03:19.953845Z",
     "iopub.status.busy": "2025-02-04T17:03:19.953557Z",
     "iopub.status.idle": "2025-02-04T17:03:43.146645Z",
     "shell.execute_reply": "2025-02-04T17:03:43.145902Z",
     "shell.execute_reply.started": "2025-02-04T17:03:19.953822Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  user\n",
      "  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "  code to check a palindrome number in C++\n",
      "  \n",
      "model\n",
      "  \n",
      "\n",
      "    Sure. The following code in C++ checks if a number is a palindrome number:\n",
      "\n",
      "```c++\n",
      "#include <iostream>\n",
      "#include <string>\n",
      "using namespace std;\n",
      "\n",
      "bool isPalindrome(int num) {\n",
      "    int reversed = 0;\n",
      "    int original = num;\n",
      "    while (original != 0) {\n",
      "        int digit = original % 10;\n",
      "        reversed = reversed * 10 + digit;\n",
      "        original /= 10;\n",
      "    }\n",
      "    return original == reversed;\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    int num = 1212;\n",
      "    if (isPalindrome(num)) {\n",
      "        cout << num << \" is a palindrome number\" << endl;\n",
      "    } else {\n",
      "        cout << num << \" is not a palindrome number\" << endl;\n",
      "    }\n",
      "    return 0;\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "result = get_completion(query=\"code to check a palindrome number in C++\", model=model, tokenizer=tokenizer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:03:49.525835Z",
     "iopub.status.busy": "2025-02-04T17:03:49.525292Z",
     "iopub.status.idle": "2025-02-04T17:03:54.665856Z",
     "shell.execute_reply": "2025-02-04T17:03:54.665190Z",
     "shell.execute_reply.started": "2025-02-04T17:03:49.525808Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0314ebec1fb4233bc8edb66e1774a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a60ac0d82a8448fb43ae87f0aefa416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/169M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac7308f4c1c4437a548d41e1c152f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'instruction', 'text', 'output'],\n",
       "    num_rows: 121959\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:04:08.263265Z",
     "iopub.status.busy": "2025-02-04T17:04:08.262653Z",
     "iopub.status.idle": "2025-02-04T17:04:08.631725Z",
     "shell.execute_reply": "2025-02-04T17:04:08.631040Z",
     "shell.execute_reply.started": "2025-02-04T17:04:08.263234Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>instruction</th>\n",
       "      <th>text</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>Create a function to calculate the sum of a se...</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td># Python code\\ndef sum_sequence(sequence):\\n  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>str1 = \"Hello \"\\nstr2 = \"world\"</td>\n",
       "      <td>Develop a function that will add two strings</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>def add_strings(str1, str2):\\n    \"\"\"This func...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>Design a data structure in C++ to store inform...</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>#include &lt;map&gt;\\n#include &lt;string&gt;\\n\\nclass Gro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[3, 1, 4, 5, 9, 0]</td>\n",
       "      <td>Implement a sorting algorithm to sort a given ...</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>def bubble_sort(arr):\\n    n = len(arr)\\n \\n  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not applicable</td>\n",
       "      <td>Design a Swift application for tracking expens...</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>import UIKit\\n\\nclass ExpenseViewController: U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>Create a REST API to convert a UNIX timestamp ...</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>&lt;?php\\n$timestamp = $_GET['timestamp'];\\n\\nif(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>website: www.example.com \\ndata to crawl: phon...</td>\n",
       "      <td>Generate a Python code for crawling a website ...</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>import requests\\nimport re\\n\\ndef crawl_websit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>Create a Python list comprehension to get the ...</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>[x*x for x in [1, 2, 3, 5, 8, 13]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>Create a MySQL query to find the most expensiv...</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>SELECT * FROM products ORDER BY price DESC LIM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Not applicable</td>\n",
       "      <td>Create a data structure in Java for storing an...</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>public class Library {\\n \\n // map of books in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0                                    [1, 2, 3, 4, 5]   \n",
       "1                    str1 = \"Hello \"\\nstr2 = \"world\"   \n",
       "2                                                      \n",
       "3                                 [3, 1, 4, 5, 9, 0]   \n",
       "4                                     Not applicable   \n",
       "5                                     Not Applicable   \n",
       "6  website: www.example.com \\ndata to crawl: phon...   \n",
       "7                                                      \n",
       "8                                                      \n",
       "9                                     Not applicable   \n",
       "\n",
       "                                         instruction  \\\n",
       "0  Create a function to calculate the sum of a se...   \n",
       "1       Develop a function that will add two strings   \n",
       "2  Design a data structure in C++ to store inform...   \n",
       "3  Implement a sorting algorithm to sort a given ...   \n",
       "4  Design a Swift application for tracking expens...   \n",
       "5  Create a REST API to convert a UNIX timestamp ...   \n",
       "6  Generate a Python code for crawling a website ...   \n",
       "7  Create a Python list comprehension to get the ...   \n",
       "8  Create a MySQL query to find the most expensiv...   \n",
       "9  Create a data structure in Java for storing an...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Below is an instruction that describes a task....   \n",
       "1  Below is an instruction that describes a task....   \n",
       "2  Below is an instruction that describes a task....   \n",
       "3  Below is an instruction that describes a task....   \n",
       "4  Below is an instruction that describes a task....   \n",
       "5  Below is an instruction that describes a task....   \n",
       "6  Below is an instruction that describes a task....   \n",
       "7  Below is an instruction that describes a task....   \n",
       "8  Below is an instruction that describes a task....   \n",
       "9  Below is an instruction that describes a task....   \n",
       "\n",
       "                                              output  \n",
       "0  # Python code\\ndef sum_sequence(sequence):\\n  ...  \n",
       "1  def add_strings(str1, str2):\\n    \"\"\"This func...  \n",
       "2  #include <map>\\n#include <string>\\n\\nclass Gro...  \n",
       "3  def bubble_sort(arr):\\n    n = len(arr)\\n \\n  ...  \n",
       "4  import UIKit\\n\\nclass ExpenseViewController: U...  \n",
       "5  <?php\\n$timestamp = $_GET['timestamp'];\\n\\nif(...  \n",
       "6  import requests\\nimport re\\n\\ndef crawl_websit...  \n",
       "7                 [x*x for x in [1, 2, 3, 5, 8, 13]]  \n",
       "8  SELECT * FROM products ORDER BY price DESC LIM...  \n",
       "9  public class Library {\\n \\n // map of books in...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset.to_pandas()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Format the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:04:11.407996Z",
     "iopub.status.busy": "2025-02-04T17:04:11.407676Z",
     "iopub.status.idle": "2025-02-04T17:04:16.357179Z",
     "shell.execute_reply": "2025-02-04T17:04:16.356496Z",
     "shell.execute_reply.started": "2025-02-04T17:04:11.407946Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n",
    "\n",
    "    :param data_point: dict: Data point\n",
    "    :return: dict: tokenzed prompt\n",
    "    \"\"\"\n",
    "    prefix_text = 'Below is an instruction that describes a task. Write a response that ' \\\n",
    "               'appropriately completes the request.\\n\\n'\n",
    "    # Samples with additional context into.\n",
    "    if data_point['input']:\n",
    "        text = f\"\"\"<start_of_turn>user {prefix_text} {data_point[\"instruction\"]} here are the inputs {data_point[\"input\"]} <end_of_turn>\\n<start_of_turn>model{data_point[\"output\"]} <end_of_turn>\"\"\"\n",
    "    # Without\n",
    "    else:\n",
    "        text = f\"\"\"<start_of_turn>user {prefix_text} {data_point[\"instruction\"]} <end_of_turn>\\n<start_of_turn>model{data_point[\"output\"]} <end_of_turn>\"\"\"\n",
    "    return text\n",
    "\n",
    "# add the \"prompt\" column in the dataset\n",
    "text_column = [generate_prompt(data_point) for data_point in dataset]\n",
    "dataset = dataset.add_column(\"prompt\", text_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.1: Tokenize the data to make it easier for model to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:04:27.810746Z",
     "iopub.status.busy": "2025-02-04T17:04:27.810454Z",
     "iopub.status.idle": "2025-02-04T17:05:00.857895Z",
     "shell.execute_reply": "2025-02-04T17:05:00.857052Z",
     "shell.execute_reply.started": "2025-02-04T17:04:27.810723Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd65d368d1f4ef49184bf9e9f4456e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/121959 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.shuffle(seed=1234)  # Shuffle dataset here\n",
    "dataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.2: Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:05:04.724520Z",
     "iopub.status.busy": "2025-02-04T17:05:04.724215Z",
     "iopub.status.idle": "2025-02-04T17:05:04.765316Z",
     "shell.execute_reply": "2025-02-04T17:05:04.764649Z",
     "shell.execute_reply.started": "2025-02-04T17:05:04.724497Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:05:07.645847Z",
     "iopub.status.busy": "2025-02-04T17:05:07.645546Z",
     "iopub.status.idle": "2025-02-04T17:05:07.649907Z",
     "shell.execute_reply": "2025-02-04T17:05:07.648865Z",
     "shell.execute_reply.started": "2025-02-04T17:05:07.645822Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input', 'instruction', 'text', 'output', 'prompt', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 24392\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Apply LoRA (Low Rank Adaptation)\n",
    "Magic with peft! \n",
    "-> Load a PeftModel and specify that we are going to use low-rank adapters (LoRA) using get_peft_model utility function and  the prepare_model_for_kbit_training method from PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:05:10.775782Z",
     "iopub.status.busy": "2025-02-04T17:05:10.775476Z",
     "iopub.status.idle": "2025-02-04T17:05:10.818518Z",
     "shell.execute_reply": "2025-02-04T17:05:10.817910Z",
     "shell.execute_reply.started": "2025-02-04T17:05:10.775736Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:05:13.018151Z",
     "iopub.status.busy": "2025-02-04T17:05:13.017813Z",
     "iopub.status.idle": "2025-02-04T17:05:13.023361Z",
     "shell.execute_reply": "2025-02-04T17:05:13.022399Z",
     "shell.execute_reply.started": "2025-02-04T17:05:13.018123Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GemmaForCausalLM(\n",
      "  (model): GemmaModel(\n",
      "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-17): 18 x GemmaDecoderLayer(\n",
      "        (self_attn): GemmaSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): GemmaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): GemmaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
      "          (act_fn): GELUActivation()\n",
      "        )\n",
      "        (input_layernorm): GemmaRMSNorm()\n",
      "        (post_attention_layernorm): GemmaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): GemmaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:05:16.572587Z",
     "iopub.status.busy": "2025-02-04T17:05:16.572274Z",
     "iopub.status.idle": "2025-02-04T17:05:16.577491Z",
     "shell.execute_reply": "2025-02-04T17:05:16.576719Z",
     "shell.execute_reply.started": "2025-02-04T17:05:16.572558Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#to determine all the linear layerss while using bitsandbytes\n",
    "import bitsandbytes as bnb\n",
    "def find_all_linear_names(model):\n",
    "  cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "  lora_module_names = set()\n",
    "  for name, module in model.named_modules():\n",
    "    if isinstance(module, cls):\n",
    "      names = name.split('.')\n",
    "      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "      lora_module_names.remove('lm_head')\n",
    "  return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:05:19.741254Z",
     "iopub.status.busy": "2025-02-04T17:05:19.740923Z",
     "iopub.status.idle": "2025-02-04T17:05:19.745859Z",
     "shell.execute_reply": "2025-02-04T17:05:19.745211Z",
     "shell.execute_reply.started": "2025-02-04T17:05:19.741227Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['down_proj', 'q_proj', 'gate_proj', 'k_proj', 'o_proj', 'v_proj', 'up_proj']\n"
     ]
    }
   ],
   "source": [
    "modules = find_all_linear_names(model)\n",
    "print(modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:09:07.047096Z",
     "iopub.status.busy": "2025-02-04T17:09:07.046736Z",
     "iopub.status.idle": "2025-02-04T17:09:07.596938Z",
     "shell.execute_reply": "2025-02-04T17:09:07.595994Z",
     "shell.execute_reply.started": "2025-02-04T17:09:07.047065Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32, #Adjust according to your resources\n",
    "    lora_alpha=16,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:09:08.848488Z",
     "iopub.status.busy": "2025-02-04T17:09:08.848192Z",
     "iopub.status.idle": "2025-02-04T17:09:08.856718Z",
     "shell.execute_reply": "2025-02-04T17:09:08.855760Z",
     "shell.execute_reply.started": "2025-02-04T17:09:08.848466Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 39223296 | total: 2545395712 | Percentage: 1.5410%\n"
     ]
    }
   ],
   "source": [
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Setup for the Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning with QLoRA and Supervised Fine-Tuning using SFTT Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:36:57.577435Z",
     "iopub.status.busy": "2025-02-04T17:36:57.577150Z",
     "iopub.status.idle": "2025-02-04T17:37:00.386873Z",
     "shell.execute_reply": "2025-02-04T17:37:00.386243Z",
     "shell.execute_reply.started": "2025-02-04T17:36:57.577412Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # Fix for padding side warning\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    dataset_text_field=\"prompt\",\n",
    "    peft_config=lora_config,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=2,  # Increased Batch size\n",
    "        gradient_accumulation_steps=2,  # Adjusted accumulation\n",
    "        warmup_ratio=0.03,  \n",
    "        max_steps=10, #Adjust accordingly\n",
    "        learning_rate=1e-4,  # Stable LR\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"adamw_bnb_8bit\",  # Efficient optimizer\n",
    "        save_strategy=\"epoch\",\n",
    "        fp16=True,  # Enable mixed precision for speedup\n",
    "        report_to = \"none\" #can change it to \"wandb\" to log results there\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:29:29.774677Z",
     "iopub.status.busy": "2025-02-04T17:29:29.774291Z",
     "iopub.status.idle": "2025-02-04T17:29:29.780232Z",
     "shell.execute_reply": "2025-02-04T17:29:29.779321Z",
     "shell.execute_reply.started": "2025-02-04T17:29:29.774645Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.device_count())  # Should return 2 (since you have T4 x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:57:32.584351Z",
     "iopub.status.busy": "2025-02-04T17:57:32.584018Z",
     "iopub.status.idle": "2025-02-04T17:57:32.588241Z",
     "shell.execute_reply": "2025-02-04T17:57:32.587376Z",
     "shell.execute_reply.started": "2025-02-04T17:57:32.584320Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Start the Training!ğŸ˜‡â¤ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:29:35.066468Z",
     "iopub.status.busy": "2025-02-04T17:29:35.066174Z",
     "iopub.status.idle": "2025-02-04T17:29:35.070198Z",
     "shell.execute_reply": "2025-02-04T17:29:35.069398Z",
     "shell.execute_reply.started": "2025-02-04T17:29:35.066446Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:31:32.894570Z",
     "iopub.status.busy": "2025-02-04T17:31:32.894248Z",
     "iopub.status.idle": "2025-02-04T17:31:32.898424Z",
     "shell.execute_reply": "2025-02-04T17:31:32.897504Z",
     "shell.execute_reply.started": "2025-02-04T17:31:32.894544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working on Kaggle, in case you wish to start the training again, with changed parameters, then run the command first (added just below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:41:54.854875Z",
     "iopub.status.busy": "2025-02-04T17:41:54.854565Z",
     "iopub.status.idle": "2025-02-04T17:41:55.108284Z",
     "shell.execute_reply": "2025-02-04T17:41:55.107289Z",
     "shell.execute_reply.started": "2025-02-04T17:41:54.854851Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#rm -rf outputs/checkpoint-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:42:17.394279Z",
     "iopub.status.busy": "2025-02-04T17:42:17.393903Z",
     "iopub.status.idle": "2025-02-04T17:45:23.842936Z",
     "shell.execute_reply": "2025-02-04T17:45:23.842282Z",
     "shell.execute_reply.started": "2025-02-04T17:42:17.394249Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 02:26, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.450600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.352900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.053300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.039100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.332700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.344200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.344200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.210900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.088800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.303700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=1.2520320177078248, metrics={'train_runtime': 185.9013, 'train_samples_per_second': 0.43, 'train_steps_per_second': 0.054, 'total_flos': 278185265479680.0, 'train_loss': 1.2520320177078248, 'epoch': 0.0})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.use_cache = False  # Disable cache for training\n",
    "trainer.train()  # Start fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Share the Adapters on the ğŸ¤— Hub! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:52:52.227426Z",
     "iopub.status.busy": "2025-02-04T17:52:52.227109Z",
     "iopub.status.idle": "2025-02-04T17:52:52.231184Z",
     "shell.execute_reply": "2025-02-04T17:52:52.230284Z",
     "shell.execute_reply.started": "2025-02-04T17:52:52.227398Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "new_model = \"gemma-2B-Instruct-Finetune-Sagar\" #Name of the model you will be pushing to huggingface model hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:52:53.945242Z",
     "iopub.status.busy": "2025-02-04T17:52:53.944799Z",
     "iopub.status.idle": "2025-02-04T17:52:54.195078Z",
     "shell.execute_reply": "2025-02-04T17:52:54.194115Z",
     "shell.execute_reply.started": "2025-02-04T17:52:53.945199Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:52:55.748354Z",
     "iopub.status.busy": "2025-02-04T17:52:55.748033Z",
     "iopub.status.idle": "2025-02-04T17:53:20.476817Z",
     "shell.execute_reply": "2025-02-04T17:53:20.475769Z",
     "shell.execute_reply.started": "2025-02-04T17:52:55.748326Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a3f6a444c340bd9dd51b140335880a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "merged_model= PeftModel.from_pretrained(base_model, new_model)\n",
    "merged_model= merged_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save_pretrained(\"merged_model\",safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"merged_model\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:53:22.743068Z",
     "iopub.status.busy": "2025-02-04T17:53:22.742713Z",
     "iopub.status.idle": "2025-02-04T17:55:39.314660Z",
     "shell.execute_reply": "2025-02-04T17:55:39.313780Z",
     "shell.execute_reply.started": "2025-02-04T17:53:22.743036Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a29b67b6c484290a57c1d8e9c3f3f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133752a1b7ac413a8f35654ea2a48eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38af74102824b86ace698341ae44f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ba214d83ba426ea7be9b2d6a8d04de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9df261604e454eb80007c3e9fd6f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246883ac31724b9c9f0ccc1d862459e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7374e3161c14d979f34f0204d34f5b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/sagarvk24/gemma-2B-Instruct-Finetune-Sagar/commit/8c8b4074255d4a7cbdaa4064b2d841fa629bea5f', commit_message='Upload tokenizer', commit_description='', oid='8c8b4074255d4a7cbdaa4064b2d841fa629bea5f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/sagarvk24/gemma-2B-Instruct-Finetune-Sagar', endpoint='https://huggingface.co', repo_type='model', repo_id='sagarvk24/gemma-2B-Instruct-Finetune-Sagar'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push the model and tokenizer to the Hugging Face Model Hub\n",
    "merged_model.push_to_hub(new_model, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To avoid Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T17:56:13.861644Z",
     "iopub.status.busy": "2025-02-04T17:56:13.861311Z",
     "iopub.status.idle": "2025-02-04T17:56:13.865346Z",
     "shell.execute_reply": "2025-02-04T17:56:13.864547Z",
     "shell.execute_reply.started": "2025-02-04T17:56:13.861615Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Test on Fine-Tuned ModelğŸ”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T18:05:05.742498Z",
     "iopub.status.busy": "2025-02-04T18:05:05.742143Z",
     "iopub.status.idle": "2025-02-04T18:05:05.746168Z",
     "shell.execute_reply": "2025-02-04T18:05:05.745266Z",
     "shell.execute_reply.started": "2025-02-04T18:05:05.742468Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Empty the CUDA cache before inference\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# # Adjust the max tokens (reduce for memory efficiency)\n",
    "# max_new_tokens = 50  # You can adjust this value based on your GPU memory\n",
    "\n",
    "# # Use mixed precision (FP16) if supported by your model\n",
    "# from torch.amp import autocast\n",
    "\n",
    "# # Modify the inference call with proper generation parameters\n",
    "# def get_completion(query, model, tokenizer):\n",
    "#     # Tokenize the input query\n",
    "#     inputs = tokenizer(query, return_tensors=\"pt\")\n",
    "\n",
    "#     # Move the inputs to the GPU\n",
    "#     input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    \n",
    "#     # Perform inference with mixed precision\n",
    "#     with torch.no_grad():  # No gradients needed for inference\n",
    "#         with autocast(device_type='cuda'):  # Specify 'cuda' for mixed precision\n",
    "#             generated_ids = model.generate(input_ids=input_ids, \n",
    "#                                            max_new_tokens=max_new_tokens, \n",
    "#                                            do_sample=True, \n",
    "#                                            pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "#     # Decode and return the result\n",
    "#     return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# # Run the function\n",
    "# result = get_completion(query=\"code to print Hello in Python\", model=merged_model, tokenizer=tokenizer)\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T18:05:22.493083Z",
     "iopub.status.busy": "2025-02-04T18:05:22.492764Z",
     "iopub.status.idle": "2025-02-04T18:05:22.496509Z",
     "shell.execute_reply": "2025-02-04T18:05:22.495552Z",
     "shell.execute_reply.started": "2025-02-04T18:05:22.493057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# result = get_completion(query=\"code the fibonacci series in python using reccursion\", model=merged_model, tokenizer=tokenizer)\n",
    "# print(result)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
