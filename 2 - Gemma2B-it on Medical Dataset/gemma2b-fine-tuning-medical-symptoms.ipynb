{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10664034,"sourceType":"datasetVersion","datasetId":6604458}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemma-header.width-1200.format-webp.webp\" width=\"100%\">\n\n## Instruct Fine-tuning [Gemma](https://blog.google/technology/developers/gemma-open-models/) using qLora and Supervise Finetuning\n\nThis is a comprahensive notebook and tutorial on how to fine tune the `gemma-2b-it` Model\n\nAll the code will be available on my Github. Do drop by and give a follow and a star⭐.\\\n[sagarvk24](https://github.com/sagarvk24)\n\\\n[Github Code](https://github.com/sagarvk24/Fine-Tune-LLMs-.git)","metadata":{}},{"cell_type":"markdown","source":"Check for GPU (In my case, using Kaggle's T4 x 2)","metadata":{}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-02-04T20:12:57.614620Z","iopub.execute_input":"2025-02-04T20:12:57.614895Z","iopub.status.idle":"2025-02-04T20:12:57.830671Z","shell.execute_reply.started":"2025-02-04T20:12:57.614872Z","shell.execute_reply":"2025-02-04T20:12:57.829895Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Tue Feb  4 20:12:57 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   49C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   47C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Step 1: Install the Dependencies","metadata":{}},{"cell_type":"code","source":"!pip3 install -q -U bitsandbytes==0.42.0 #To load the quantized version of the model\n!pip3 install -q -U peft==0.8.2 #For Parameter-Efficient Fine Tuning\n!pip3 install -q -U trl==0.7.10 #Supervised Fine Tuning \n!pip3 install -q -U accelerate==0.27.1\n!pip3 install -q -U datasets==2.17.0 #for loading the datasets\n!pip3 install -q -U transformers==4.38.0 #for main processing and training ","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:13:02.411082Z","iopub.execute_input":"2025-02-04T20:13:02.411373Z","iopub.status.idle":"2025-02-04T20:13:42.526991Z","shell.execute_reply.started":"2025-02-04T20:13:02.411350Z","shell.execute_reply":"2025-02-04T20:13:42.525943Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.7/115.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.7/279.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2023.10.0 which is incompatible.\ns3fs 2024.9.0 requires fsspec==2024.9.0.*, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"We will load the model using QLoRA Quantization to reduce the usage of memory, as it is very difficult to load such big LLM and fine tune. ","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, #Quantization\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\", \n    bnb_4bit_compute_dtype=torch.bfloat16 #To avoid information loss while fine tuning\n)","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:13:55.046847Z","iopub.execute_input":"2025-02-04T20:13:55.047229Z","iopub.status.idle":"2025-02-04T20:13:59.111653Z","shell.execute_reply.started":"2025-02-04T20:13:55.047199Z","shell.execute_reply":"2025-02-04T20:13:59.111026Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Step 2: Hugging Face: Log In and Access Key Setup ","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:14:06.962669Z","iopub.execute_input":"2025-02-04T20:14:06.963144Z","iopub.status.idle":"2025-02-04T20:14:06.986762Z","shell.execute_reply.started":"2025-02-04T20:14:06.963113Z","shell.execute_reply":"2025-02-04T20:14:06.985757Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ba7faa2da77474fb33ea2b341f66b9a"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"#In case using Google Colab\n#Paste the access token in Colab's Environment \n\n#import os\n#from google.colab import userdata\n#os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 3: Load the Model","metadata":{}},{"cell_type":"code","source":"#Difference between base model and 'it' model is that, base model is the most raw form of the LLM model. The output is produced in a very raw format, whereas 'it' model produces outputs in a very \n\n# model_id = \"google/gemma-7b-it\"\n# model_id = \"google/gemma-7b\"\nmodel_id = \"google/gemma-2b-it\"\n# model_id = \"google/gemma-2b\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\ntokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:14:15.737262Z","iopub.execute_input":"2025-02-04T20:14:15.737541Z","iopub.status.idle":"2025-02-04T20:16:31.107662Z","shell.execute_reply.started":"2025-02-04T20:14:15.737520Z","shell.execute_reply":"2025-02-04T20:16:31.106684Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e72673cb715c415a93400b4170ed90a7"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12117120649d4f71b8ea2036dba7896a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e6954ca9f8c416888c0130b04491d59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99b994f11fd4497e8e8be451980b1c58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c81afc90c3c427292ecfd8011a20f57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41483b97595248b6afbb3bf0d0fa2cc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8935a8dff2c44fa99ba9407f0f4760a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8a28a33b19a4151bc8e2c4ccddf1562"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90e3d6cc801248c0851190b49308804b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8db34641cbb948b5b5ded2020a18e0e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b7c3251e4814bd581934d316e100ea7"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"def get_completion(query: str, model, tokenizer) -> str:\n  device = \"cuda:0\"\n\n  prompt_template = \"\"\"\n  <start_of_turn>user\n  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n  {query}\n  <end_of_turn>\\n<start_of_turn>model\n  \n\n  \"\"\"\n  prompt = prompt_template.format(query=query)\n\n  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n\n  model_inputs = encodeds.to(device)\n\n\n  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n  # decoded = tokenizer.batch_decode(generated_ids)\n  decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n  return (decoded)","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:16:36.059537Z","iopub.execute_input":"2025-02-04T20:16:36.059846Z","iopub.status.idle":"2025-02-04T20:16:36.064567Z","shell.execute_reply.started":"2025-02-04T20:16:36.059819Z","shell.execute_reply":"2025-02-04T20:16:36.063695Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Testing ","metadata":{}},{"cell_type":"code","source":"result = get_completion(query=\"code to check a number is even or odd, in C++\", model=model, tokenizer=tokenizer)\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:16:52.840636Z","iopub.execute_input":"2025-02-04T20:16:52.840913Z","iopub.status.idle":"2025-02-04T20:17:12.923306Z","shell.execute_reply.started":"2025-02-04T20:16:52.840893Z","shell.execute_reply":"2025-02-04T20:17:12.922392Z"},"trusted":true},"outputs":[{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"\n  user\n  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n  code to check a number is even or odd, in C++\n  \nmodel\n  \n\n    ```c++\n  #include <iostream>\n  #include <cmath>\n\n  using namespace std;\n\n  int main() {\n    int number;\n\n    // Get the number from the user\n    cout << \"Enter a number: \";\n    cin >> number;\n\n    // Check if the number is even\n    if (number % 2 == 0) {\n      cout << \"The number \" << number << \" is even\" << endl;\n    } else {\n      cout << \"The number \" << number << \" is odd\" << endl;\n    }\n\n    return 0;\n  }\n  ```\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Step 4: Load the Dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# disease_symptoms_path = kagglehub.dataset_download('sagarvk18/disease-symptoms')\ndataset = load_dataset(r\"/kaggle/input/disease-symptoms\", split=\"train\")\ndataset","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:19:38.957101Z","iopub.execute_input":"2025-02-04T20:19:38.957499Z","iopub.status.idle":"2025-02-04T20:19:40.464058Z","shell.execute_reply.started":"2025-02-04T20:19:38.957467Z","shell.execute_reply":"2025-02-04T20:19:40.463390Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4de707f235b144899c7eda9699bb9ac3"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['instruction', 'input', 'output'],\n    num_rows: 349\n})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"df = dataset.to_pandas()\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:19:45.895221Z","iopub.execute_input":"2025-02-04T20:19:45.895536Z","iopub.status.idle":"2025-02-04T20:19:45.928482Z","shell.execute_reply.started":"2025-02-04T20:19:45.895510Z","shell.execute_reply":"2025-02-04T20:19:45.927650Z"},"trusted":true},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                                         instruction  \\\n0  Diagnose the patient based on the provided sym...   \n1  Diagnose the patient based on the provided sym...   \n2  Diagnose the patient based on the provided sym...   \n3  Diagnose the patient based on the provided sym...   \n4  Diagnose the patient based on the provided sym...   \n5  Diagnose the patient based on the provided sym...   \n6  Diagnose the patient based on the provided sym...   \n7  Diagnose the patient based on the provided sym...   \n8  Diagnose the patient based on the provided sym...   \n9  Diagnose the patient based on the provided sym...   \n\n                                               input    output  \n0  Patient has Influenza with symptoms: Fever (Ye...  Positive  \n1  Patient has Common Cold with symptoms: Fever (...  Negative  \n2  Patient has Eczema with symptoms: Fever (No), ...  Negative  \n3  Patient has Asthma with symptoms: Fever (Yes),...  Positive  \n4  Patient has Asthma with symptoms: Fever (Yes),...  Positive  \n5  Patient has Eczema with symptoms: Fever (Yes),...  Positive  \n6  Patient has Influenza with symptoms: Fever (Ye...  Positive  \n7  Patient has Influenza with symptoms: Fever (Ye...  Positive  \n8  Patient has Hyperthyroidism with symptoms: Fev...  Negative  \n9  Patient has Hyperthyroidism with symptoms: Fev...  Negative  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>instruction</th>\n      <th>input</th>\n      <th>output</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Diagnose the patient based on the provided sym...</td>\n      <td>Patient has Influenza with symptoms: Fever (Ye...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Diagnose the patient based on the provided sym...</td>\n      <td>Patient has Common Cold with symptoms: Fever (...</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Diagnose the patient based on the provided sym...</td>\n      <td>Patient has Eczema with symptoms: Fever (No), ...</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Diagnose the patient based on the provided sym...</td>\n      <td>Patient has Asthma with symptoms: Fever (Yes),...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Diagnose the patient based on the provided sym...</td>\n      <td>Patient has Asthma with symptoms: Fever (Yes),...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Diagnose the patient based on the provided sym...</td>\n      <td>Patient has Eczema with symptoms: Fever (Yes),...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Diagnose the patient based on the provided sym...</td>\n      <td>Patient has Influenza with symptoms: Fever (Ye...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Diagnose the patient based on the provided sym...</td>\n      <td>Patient has Influenza with symptoms: Fever (Ye...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Diagnose the patient based on the provided sym...</td>\n      <td>Patient has Hyperthyroidism with symptoms: Fev...</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Diagnose the patient based on the provided sym...</td>\n      <td>Patient has Hyperthyroidism with symptoms: Fev...</td>\n      <td>Negative</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"# Step 5: Format the Dataset","metadata":{}},{"cell_type":"code","source":"def generate_prompt(data_point):\n    \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n\n    :param data_point: dict: Data point\n    :return: dict: tokenzed prompt\n    \"\"\"\n    prefix_text = 'Below is an instruction that describes a task. Write a response that ' \\\n               'appropriately completes the request.\\n\\n'\n    # Samples with additional context into.\n    if data_point['input']:\n        text = f\"\"\"<start_of_turn>user {prefix_text} {data_point[\"instruction\"]} here are the inputs {data_point[\"input\"]} <end_of_turn>\\n<start_of_turn>model{data_point[\"output\"]} <end_of_turn>\"\"\"\n    # Without\n    else:\n        text = f\"\"\"<start_of_turn>user {prefix_text} {data_point[\"instruction\"]} <end_of_turn>\\n<start_of_turn>model{data_point[\"output\"]} <end_of_turn>\"\"\"\n    return text\n\n# add the \"prompt\" column in the dataset\ntext_column = [generate_prompt(data_point) for data_point in dataset]\ndataset = dataset.add_column(\"prompt\", text_column)","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:19:57.761516Z","iopub.execute_input":"2025-02-04T20:19:57.761803Z","iopub.status.idle":"2025-02-04T20:19:57.786800Z","shell.execute_reply.started":"2025-02-04T20:19:57.761779Z","shell.execute_reply":"2025-02-04T20:19:57.785947Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Step 5.1: Tokenize the data to make it easier for model to understand","metadata":{}},{"cell_type":"code","source":"dataset = dataset.shuffle(seed=1234)  # Shuffle dataset here\ndataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:20:03.402016Z","iopub.execute_input":"2025-02-04T20:20:03.402345Z","iopub.status.idle":"2025-02-04T20:20:03.857636Z","shell.execute_reply.started":"2025-02-04T20:20:03.402318Z","shell.execute_reply":"2025-02-04T20:20:03.856667Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f289899a45b2462b9369af9ce8d65597"}},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"## Step 5.2: Split the Dataset","metadata":{}},{"cell_type":"code","source":"dataset = dataset.train_test_split(test_size=0.2)\ntrain_data = dataset[\"train\"]\ntest_data = dataset[\"test\"]","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:20:08.762761Z","iopub.execute_input":"2025-02-04T20:20:08.763095Z","iopub.status.idle":"2025-02-04T20:20:08.776276Z","shell.execute_reply.started":"2025-02-04T20:20:08.763067Z","shell.execute_reply":"2025-02-04T20:20:08.775442Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"print(test_data)","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:20:10.814302Z","iopub.execute_input":"2025-02-04T20:20:10.814586Z","iopub.status.idle":"2025-02-04T20:20:10.819125Z","shell.execute_reply.started":"2025-02-04T20:20:10.814563Z","shell.execute_reply":"2025-02-04T20:20:10.818234Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['instruction', 'input', 'output', 'prompt', 'input_ids', 'attention_mask'],\n    num_rows: 70\n})\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# Step 6: Apply LoRA (Low Rank Adaptation)\nMagic with peft! \n-> Load a PeftModel and specify that we are going to use low-rank adapters (LoRA) using get_peft_model utility function and  the prepare_model_for_kbit_training method from PEFT.","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:20:16.049552Z","iopub.execute_input":"2025-02-04T20:20:16.049864Z","iopub.status.idle":"2025-02-04T20:20:16.093181Z","shell.execute_reply.started":"2025-02-04T20:20:16.049838Z","shell.execute_reply":"2025-02-04T20:20:16.092263Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:20:18.614146Z","iopub.execute_input":"2025-02-04T20:20:18.614446Z","iopub.status.idle":"2025-02-04T20:20:18.619845Z","shell.execute_reply.started":"2025-02-04T20:20:18.614424Z","shell.execute_reply":"2025-02-04T20:20:18.618970Z"},"trusted":true},"outputs":[{"name":"stdout","text":"GemmaForCausalLM(\n  (model): GemmaModel(\n    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n    (layers): ModuleList(\n      (0-17): 18 x GemmaDecoderLayer(\n        (self_attn): GemmaSdpaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n    )\n    (norm): GemmaRMSNorm()\n  )\n  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"#to determine all the linear layerss while using bitsandbytes\nimport bitsandbytes as bnb\ndef find_all_linear_names(model):\n  cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n  lora_module_names = set()\n  for name, module in model.named_modules():\n    if isinstance(module, cls):\n      names = name.split('.')\n      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    if 'lm_head' in lora_module_names: # needed for 16-bit\n      lora_module_names.remove('lm_head')\n  return list(lora_module_names)","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:20:23.198490Z","iopub.execute_input":"2025-02-04T20:20:23.198816Z","iopub.status.idle":"2025-02-04T20:20:23.203704Z","shell.execute_reply.started":"2025-02-04T20:20:23.198790Z","shell.execute_reply":"2025-02-04T20:20:23.202965Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"code","source":"modules = find_all_linear_names(model)\nprint(modules)","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:20:25.904671Z","iopub.execute_input":"2025-02-04T20:20:25.904963Z","iopub.status.idle":"2025-02-04T20:20:25.909826Z","shell.execute_reply.started":"2025-02-04T20:20:25.904920Z","shell.execute_reply":"2025-02-04T20:20:25.909019Z"},"trusted":true},"outputs":[{"name":"stdout","text":"['up_proj', 'down_proj', 'o_proj', 'v_proj', 'gate_proj', 'q_proj', 'k_proj']\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=32, #Adjust according to your resources\n    lora_alpha=16,\n    target_modules=modules,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:20:33.354030Z","iopub.execute_input":"2025-02-04T20:20:33.354362Z","iopub.status.idle":"2025-02-04T20:20:33.887584Z","shell.execute_reply.started":"2025-02-04T20:20:33.354338Z","shell.execute_reply":"2025-02-04T20:20:33.886839Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"trainable, total = model.get_nb_trainable_parameters()\nprint(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:20:37.195384Z","iopub.execute_input":"2025-02-04T20:20:37.195818Z","iopub.status.idle":"2025-02-04T20:20:37.207172Z","shell.execute_reply.started":"2025-02-04T20:20:37.195781Z","shell.execute_reply":"2025-02-04T20:20:37.206101Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Trainable: 39223296 | total: 2545395712 | Percentage: 1.5410%\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# Step 7: Setup for the Training","metadata":{}},{"cell_type":"markdown","source":"## Fine-Tuning with QLoRA and Supervised Fine-Tuning using SFTT Trainer","metadata":{}},{"cell_type":"code","source":"import transformers\nfrom trl import SFTTrainer\n\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"  # Fix for padding side warning\ntorch.cuda.empty_cache()\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_data,\n    eval_dataset=test_data,\n    dataset_text_field=\"prompt\",\n    peft_config=lora_config,\n    args=transformers.TrainingArguments(\n        per_device_train_batch_size=2,  # Increased Batch size\n        gradient_accumulation_steps=2,  # Adjusted accumulation\n        warmup_ratio=0.03,  \n        max_steps=20, #Adjust accordingly\n        learning_rate=1e-4,  # Stable LR\n        logging_steps=1,\n        output_dir=\"outputs\",\n        optim=\"adamw_bnb_8bit\",  # Efficient optimizer\n        save_strategy=\"epoch\",\n        fp16=True,  # Enable mixed precision for speedup\n        report_to = \"none\" #can change it to \"wandb\" to log results there\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\n","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:23:05.027026Z","iopub.execute_input":"2025-02-04T20:23:05.027344Z","iopub.status.idle":"2025-02-04T20:23:06.950700Z","shell.execute_reply.started":"2025-02-04T20:23:05.027319Z","shell.execute_reply":"2025-02-04T20:23:06.949794Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/70 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37f2fca80b324562b6f546b3aa54fb84"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:450: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())  # Should return True\nprint(torch.cuda.device_count())  # Should return 2 (since you have T4 x2)","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:20:56.853156Z","iopub.execute_input":"2025-02-04T20:20:56.853448Z","iopub.status.idle":"2025-02-04T20:20:56.857879Z","shell.execute_reply.started":"2025-02-04T20:20:56.853426Z","shell.execute_reply":"2025-02-04T20:20:56.856838Z"},"trusted":true},"outputs":[{"name":"stdout","text":"True\n2\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:21:00.182119Z","iopub.execute_input":"2025-02-04T20:21:00.182433Z","iopub.status.idle":"2025-02-04T20:21:00.185956Z","shell.execute_reply.started":"2025-02-04T20:21:00.182407Z","shell.execute_reply":"2025-02-04T20:21:00.185179Z"},"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"# Step 8: Start the Training!😇❤️","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:21:02.027680Z","iopub.execute_input":"2025-02-04T20:21:02.028001Z","iopub.status.idle":"2025-02-04T20:21:02.031726Z","shell.execute_reply.started":"2025-02-04T20:21:02.027975Z","shell.execute_reply":"2025-02-04T20:21:02.030893Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:21:04.561446Z","iopub.execute_input":"2025-02-04T20:21:04.561732Z","iopub.status.idle":"2025-02-04T20:21:04.565421Z","shell.execute_reply.started":"2025-02-04T20:21:04.561710Z","shell.execute_reply":"2025-02-04T20:21:04.564612Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"When working on Kaggle, in case you wish to start the training again, with changed parameters, then run the command first (added just below)","metadata":{}},{"cell_type":"code","source":"rm -rf outputs/checkpoint-10","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:22:59.439773Z","iopub.execute_input":"2025-02-04T20:22:59.440463Z","iopub.status.idle":"2025-02-04T20:22:59.663783Z","shell.execute_reply.started":"2025-02-04T20:22:59.440430Z","shell.execute_reply":"2025-02-04T20:22:59.662823Z"},"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":"model.config.use_cache = False  # Disable cache for training\ntrainer.train()  # Start fine-tuning","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:23:12.442536Z","iopub.execute_input":"2025-02-04T20:23:12.442839Z","iopub.status.idle":"2025-02-04T20:25:58.930970Z","shell.execute_reply.started":"2025-02-04T20:23:12.442818Z","shell.execute_reply":"2025-02-04T20:25:58.930134Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [20/20 02:38, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.550000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.505300</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.878700</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.451800</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.103200</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.892700</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.630600</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.474900</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.348600</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.264700</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>1.203300</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.123800</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>1.064900</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.017800</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.979300</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.967800</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.947500</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.919000</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.923800</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.920900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=20, training_loss=1.6084246426820754, metrics={'train_runtime': 166.0634, 'train_samples_per_second': 0.963, 'train_steps_per_second': 0.12, 'total_flos': 176806502645760.0, 'train_loss': 1.6084246426820754, 'epoch': 0.57})"},"metadata":{}}],"execution_count":31},{"cell_type":"markdown","source":"# Step 9: Share the Adapters on the 🤗 Hub! ","metadata":{}},{"cell_type":"code","source":"new_model = \"gemma-2B-Instruct-Finetune-MedicalSymptoms-Sagar\" #Name of the model you will be pushing to huggingface model hub","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:26:28.011880Z","iopub.execute_input":"2025-02-04T20:26:28.012237Z","iopub.status.idle":"2025-02-04T20:26:28.015897Z","shell.execute_reply.started":"2025-02-04T20:26:28.012208Z","shell.execute_reply":"2025-02-04T20:26:28.014975Z"},"trusted":true},"outputs":[],"execution_count":32},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:26:32.059477Z","iopub.execute_input":"2025-02-04T20:26:32.059761Z","iopub.status.idle":"2025-02-04T20:26:32.731575Z","shell.execute_reply.started":"2025-02-04T20:26:32.059740Z","shell.execute_reply":"2025-02-04T20:26:32.730891Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"base_model = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map={\"\": 0},\n)\nmerged_model= PeftModel.from_pretrained(base_model, new_model)\nmerged_model= merged_model.merge_and_unload()\n\n# Save the merged model\nmerged_model.save_pretrained(\"merged_model\",safe_serialization=True)\ntokenizer.save_pretrained(\"merged_model\")\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:26:38.094914Z","iopub.execute_input":"2025-02-04T20:26:38.095245Z","iopub.status.idle":"2025-02-04T20:27:01.988806Z","shell.execute_reply.started":"2025-02-04T20:26:38.095219Z","shell.execute_reply":"2025-02-04T20:27:01.988061Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8c59d31d05647d592133e62b8929f3b"}},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"# Push the model and tokenizer to the Hugging Face Model Hub\nmerged_model.push_to_hub(new_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:27:05.562725Z","iopub.execute_input":"2025-02-04T20:27:05.563135Z","iopub.status.idle":"2025-02-04T20:30:13.595406Z","shell.execute_reply.started":"2025-02-04T20:27:05.563097Z","shell.execute_reply":"2025-02-04T20:30:13.594612Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f42bfe187bb451bae143175e7985660"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dda4c38929643d79898a24a8739ca48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1981b2ed079e4154b006b728a8d64e43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ca5b8409d2744abbc65807886b2e2a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8d93c5908af412da22857dd96d21bf1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82f583e7b52545c99dc7bf8493257245"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e0b715c7a8942cd86c84b7dd40c2e4a"}},"metadata":{}},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/sagarvk24/gemma-2B-Instruct-Finetune-MedicalSymptoms-Sagar/commit/dc32b0ef59d072ec108911e1f118684481eb7edd', commit_message='Upload tokenizer', commit_description='', oid='dc32b0ef59d072ec108911e1f118684481eb7edd', pr_url=None, repo_url=RepoUrl('https://huggingface.co/sagarvk24/gemma-2B-Instruct-Finetune-MedicalSymptoms-Sagar', endpoint='https://huggingface.co', repo_type='model', repo_id='sagarvk24/gemma-2B-Instruct-Finetune-MedicalSymptoms-Sagar'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":35},{"cell_type":"markdown","source":"### To avoid Warnings","metadata":{}},{"cell_type":"code","source":"tokenizer.padding_side = 'left'\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:32:29.603325Z","iopub.execute_input":"2025-02-04T20:32:29.603660Z","iopub.status.idle":"2025-02-04T20:32:29.607189Z","shell.execute_reply.started":"2025-02-04T20:32:29.603635Z","shell.execute_reply":"2025-02-04T20:32:29.606366Z"},"trusted":true},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"# Step 10: Test on Fine-Tuned Model🔎","metadata":{}},{"cell_type":"code","source":"query = \"\"\"\nDiagnose the patient based on the provided symptoms and information.\n\nPatient has persistent cough and fever with symptoms: Fever (Yes), Cough (Yes), Fatigue (Moderate), Difficulty Breathing (No). \nAge: 35, Gender: Male, Blood Pressure: Normal, Cholesterol Level: High.\n\"\"\"\n\nresult = get_completion(query, model=merged_model, tokenizer=tokenizer)\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2025-02-04T20:31:21.134016Z","iopub.execute_input":"2025-02-04T20:31:21.134413Z","iopub.status.idle":"2025-02-04T20:31:26.105077Z","shell.execute_reply.started":"2025-02-04T20:31:21.134381Z","shell.execute_reply":"2025-02-04T20:31:26.104156Z"},"trusted":true},"outputs":[{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"\n  user\n  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n  \nDiagnose the patient based on the provided symptoms and information.\n\nPatient has persistent cough and fever with symptoms: Fever (Yes), Cough (Yes), Fatigue (Moderate), Difficulty Breathing (No). \nAge: 35, Gender: Male, Blood Pressure: Normal, Cholesterol Level: High.\n\n  \nmodel\n  \n\n   Analyzing the patient's symptoms and information, here's the preliminary diagnosis:\n\n  Cough with fever may be associated with various conditions. Common causes include respiratory infections such as pneumonia, bronchitis, and influenza. However, the absence of cough in this case is significant and can suggest other possibilities.\n\n   Fever with fatigue is a typical symptom of conditions like viral fevers and infections. High blood pressure and cholesterol levels might also contribute to the patient's symptoms.\n\n  Therefore, further investigations such as a chest X-ray or electrocardiogram would be necessary to confirm the initial suspicion and determine the underlying cause.\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"query = \"\"\"\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n\nDiagnose the patient based on the provided symptoms and information.\n\nImportant: Carefully consider the presence or absence of each symptom in the diagnosis.\n\nPatient has persistent cough and fever with symptoms: Fever (Yes), Cough (Yes), Fatigue (Moderate), Difficulty Breathing (No).\nAge: 35, Gender: Male, Blood Pressure: Normal, Cholesterol Level: High.\n\"\"\"\n\nresult = get_completion(query=query, model=merged_model, tokenizer=tokenizer)\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T20:32:35.907208Z","iopub.execute_input":"2025-02-04T20:32:35.907501Z","iopub.status.idle":"2025-02-04T20:32:39.395530Z","shell.execute_reply.started":"2025-02-04T20:32:35.907479Z","shell.execute_reply":"2025-02-04T20:32:39.394763Z"}},"outputs":[{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"\n  user\n  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n  \nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n\nDiagnose the patient based on the provided symptoms and information.\n\nImportant: Carefully consider the presence or absence of each symptom in the diagnosis.\n\nPatient has persistent cough and fever with symptoms: Fever (Yes), Cough (Yes), Fatigue (Moderate), Difficulty Breathing (No).\nAge: 35, Gender: Male, Blood Pressure: Normal, Cholesterol Level: High.\n\n  \nmodel\n  \n\n   **Possible Diagnosis:** Asthma\n\n  Based on the patient's symptoms, particularly the absence of difficulty breathing, the diagnosis of Asthma is most likely. Asthma is characterized by symptoms such as persistent coughing, fever, and difficulty breathing. However, without the presence of difficulty breathing, the diagnosis of asthma may be less definite. \n\nIt is important to note that this is only a preliminary diagnosis and further clinical evaluation necessary for a more accurate diagnosis is recommended.\n","output_type":"stream"}],"execution_count":39}]}